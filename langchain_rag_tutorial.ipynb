{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62667738",
   "metadata": {},
   "source": [
    "# Notebook - Langchain RAG Tutorial\n",
    "\n",
    "**Escuela Colombiana de Ingeniería Julio Garavito**\n",
    "\n",
    "**Student:** Santiago Botero García\n",
    "\n",
    "## Step 0: Setup & Imports\n",
    "\n",
    "Import all required standard and LangChain-specific dependencies used to build a Retrieval-Augmented Generation (RAG) pipeline.\n",
    "\n",
    "* `os` and `getpass` are used to securely manage environment variables, particularly API keys.\n",
    "* `bs4` (BeautifulSoup) enables selective HTML parsing when loading web content.\n",
    "* `WebBaseLoader` loads documents directly from a URL into LangChain’s `Document` format.\n",
    "* `RecursiveCharacterTextSplitter` breaks large documents into smaller, overlapping chunks suitable for embedding.\n",
    "* `tool` and `create_agent` are used to expose retrieval logic as a callable tool that an LLM-powered agent can invoke.\n",
    "\n",
    "This mirrors the LangChain docs’ emphasis on **modular primitives**: loaders, splitters, tools, and agents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5835bc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%pip install langchain langchain-text-splitters langchain-community bs4 chromadb\n",
    "\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "import bs4\n",
    "from langchain.agents import create_agent\n",
    "from langchain.tools import tool\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f755b16",
   "metadata": {},
   "source": [
    "This block ensures the required LLM API key is available before running the pipeline.\n",
    "\n",
    "* It first checks whether `OPENAI_API_KEY` is already set in the environment.\n",
    "* If not, it securely prompts the user at runtime without echoing the key to the console.\n",
    "\n",
    "The LangChain documentation recommends this approach to avoid hard-coding secrets and to keep notebooks and scripts safe for sharing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f5a7cb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f629be98",
   "metadata": {},
   "source": [
    "## Step 1: Load and Chunk Documents\n",
    "\n",
    "### Web Document Loading\n",
    "\n",
    "This section loads external unstructured data (a blog post) into the RAG pipeline.\n",
    "\n",
    "* `WebBaseLoader` fetches the webpage and converts it into LangChain `Document` objects.\n",
    "* `SoupStrainer` limits parsing to relevant HTML sections, reducing noise such as navigation bars or footers.\n",
    "* This aligns with the LangChain RAG tutorial’s guidance to **control input quality before embedding**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe36bc4c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee27a0ba",
   "metadata": {},
   "source": [
    "### Loading Documents into Memory\n",
    "\n",
    "The webpage content is loaded and normalized into LangChain’s `Document` format, which includes:\n",
    "\n",
    "* `page_content` (the text)\n",
    "* `metadata` (source URL and other attributes)\n",
    "\n",
    "This abstraction allows downstream components (splitters, vector stores, retrievers) to work consistently across different data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c088cbf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "docs = loader.load()\n",
    "print(f\"[+] Loaded {len(docs)} docs — characters: {len(docs[0].page_content)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6349bb",
   "metadata": {},
   "source": [
    "### Text Chunking\n",
    "\n",
    "Large documents are split into smaller overlapping chunks to optimize retrieval and embedding quality.\n",
    "\n",
    "* `chunk_size=1000` ensures each chunk fits comfortably within model context limits.\n",
    "* `chunk_overlap=200` preserves semantic continuity between chunks.\n",
    "* The recursive strategy attempts to split on natural boundaries (paragraphs, sentences) when possible.\n",
    "\n",
    "This step reflects the RAG principle that **retrieval happens at the chunk level, not the document level**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a20608a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "print(f\"[+] Split into {len(all_splits)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96cca7d",
   "metadata": {},
   "source": [
    "## Step 2: Vector Store & Retrieval Setup\n",
    "\n",
    "This section introduces the components responsible for semantic search:\n",
    "\n",
    "* `OpenAIEmbeddings` converts text chunks into high-dimensional vectors.\n",
    "* `Chroma` is a lightweight, local vector database used to store and search those embeddings.\n",
    "\n",
    "LangChain’s RAG tutorial highlights that vector stores are **pluggable**, and Chroma is used here for simplicity and fast prototyping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11dd6d2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748f404a",
   "metadata": {},
   "source": [
    "### Creating Embeddings and Vector Store\n",
    "\n",
    "* Each text chunk is embedded using an OpenAI embedding model.\n",
    "* The embeddings are stored in Chroma, enabling similarity-based retrieval.\n",
    "* This forms the knowledge base that the RAG system will query at runtime.\n",
    "\n",
    "At this point, the pipeline has transformed raw web content into a searchable semantic index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84f66ad",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "vector_store = Chroma.from_documents(all_splits, embedding=embeddings)\n",
    "\n",
    "print(\"[+] Created vector store with embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2407539f",
   "metadata": {},
   "source": [
    "## Step 3: RAG Tool for the Agent\n",
    "\n",
    "This function exposes retrieval as a **LangChain tool**, allowing an agent to explicitly decide when and how to perform semantic search based on the user query. It executes a similarity search against the vector store to retrieve the top `k=4` most relevant document chunks, grounding the LLM’s response in external knowledge rather than hidden context. Retrieved documents are then serialized into a structured text format suitable for LLM consumption, while the original documents are returned separately as artifacts. This design enables transparent source inspection, easier debugging, and citation-style workflows, aligning with the tutorial’s pattern of making retrieval a first-class, observable action in the agent loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d18cd9e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve_context(query: str):\n",
    "    \"\"\"Retrieve relevant document chunks from the vector store.\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=4)\n",
    "    # Serialize results for the LLM\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        f\"Source: {doc.metadata}\\nContent: {doc.page_content}\"\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs\n",
    "\n",
    "# Create a list of tools available to the agent\n",
    "tools = [retrieve_context]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262194a4",
   "metadata": {},
   "source": [
    "## Step 4: Create the Agent\n",
    "\n",
    "The system prompt defines the agent’s role and behavior.\n",
    "\n",
    "* It explicitly instructs the agent to rely on retrieval rather than hallucation.\n",
    "* This aligns with LangChain’s recommendation to **steer agent behavior via prompts**, not just code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b839af",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"You are a retrieval-augmented generation agent. \"\n",
    "    \"Use the provided retrieval tool to answer user queries.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffb6b8f",
   "metadata": {},
   "source": [
    "This creates a LangChain agent that:\n",
    "\n",
    "* Uses the default chat model configured via environment variables\n",
    "* Has access to the retrieval tool\n",
    "* Can reason about when to call the tool versus when to answer directly\n",
    "\n",
    "This is the core orchestration layer of the RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4bb5de",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "agent = create_agent(\n",
    "    model=None,\n",
    "    tools=tools,\n",
    "    system_prompt=system_prompt\n",
    ")\n",
    "print(\"[+] Agent created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971fb061",
   "metadata": {},
   "source": [
    "## Step 5: Running the Agent\n",
    "\n",
    "A sample user query is provided to demonstrate the full RAG loop:\n",
    "\n",
    "1. The agent receives the question\n",
    "2. It invokes the retrieval tool\n",
    "3. Retrieved context is injected\n",
    "4. The final answer is generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9204679a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "query = \"What is task decomposition?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfbc7d6",
   "metadata": {},
   "source": [
    "The agent is executed in streaming mode, allowing you to observe intermediate reasoning and tool usage step by step.\n",
    "\n",
    "This reflects the LangChain tutorial’s focus on **inspectability and transparency**, making RAG systems easier to debug and understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84221c40",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"\\nQuery: {query}\\n\")\n",
    "for step in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    # Pretty print each message\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
